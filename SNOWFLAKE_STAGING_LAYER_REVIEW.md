# Snowflake Staging Layer Design Review

**Date:** 2025-11-20
**Review Question:** Should we only use views/materialized views in the staging layer per Snowflake best practices?
**Answer:** No - physical tables are appropriate for your use case, but Dynamic Tables offer a modern alternative.

---

## Executive Summary

After reviewing your staging layer design against Snowflake best practices:

### âœ… Your Current Design is Sound

Your **physical tables with stored procedures** approach is **architecturally correct** for your complex OMS/VEMS integration requirements. This aligns with Snowflake's medallion architecture guidance for Silver layers with:
- Complex entity matching and deduplication
- System-of-record conflict resolution
- Comprehensive audit requirements
- Batch processing patterns

### ğŸš€ Dynamic Tables: Modern Alternative Implemented

As a brand new solution, we've implemented a **Dynamic Tables approach** for simpler components:
- **Facilities** (reference data, low conflict)
- **Appointment Events** (fact table, straightforward)
- **QA Events** (fact table, single-source)
- **Data Quality Dashboard** (real-time metrics)

This demonstrates modern Snowflake capabilities while keeping complex veteran/evaluator processing in stored procedures.

---

## Question Analysis

### Should We Only Use Views/Materialized Views?

**Short Answer:** No.

**Snowflake Guidance:** The staging/silver layer can use **tables, views, OR materialized views** depending on:
- Transformation complexity
- Update frequency
- Query patterns
- Audit requirements

There is **no one-size-fits-all rule** mandating views/materialized views for staging layers.

---

## Your Current Architecture Assessment

### What You Have: Physical Tables + Stored Procedures

```
ODS_RAW (Bronze)
    â†“ [Stored Procedures]
REFERENCE (Crosswalks)
    â†“ [Stored Procedures]
STAGING (Physical Tables)
    â†“ [ETL]
WAREHOUSE (Gold)
```

### Why This is Correct for Your Use Case

| Requirement | Why Physical Tables | Why Not Views |
|-------------|---------------------|---------------|
| **Complex Transformations** | Entity matching, conflict resolution, DQ scoring | Too expensive to recompute on every query |
| **Batch Processing** | DELETE+INSERT with batch_id for idempotency | Can't persist batch_id in views |
| **Audit Trail** | Write conflict logs, maintain crosswalks | Can't write to audit tables from views |
| **Performance** | Frequently queried by downstream consumers | Would recompute joins/logic every query |
| **Change Detection** | Store source_record_hash between batches | No state persistence in views |

### Snowflake Medallion Architecture Alignment

Your design follows established patterns:

âœ… **Bronze/ODS**: Raw ingestion (physical tables)
âœ… **Silver/Staging**: Conformed entities (physical tables with complex logic)
âœ… **Gold/Warehouse**: Dimensional models (physical tables)
âœ… **Marts**: Business views (views on top of gold)

---

## Alternatives Considered

### Option 1: Regular Views âŒ

**Problem:** Too expensive to recompute on every query

```sql
CREATE VIEW stg_veterans AS
SELECT /* 200+ lines of merge logic, joins, DQ scoring */
```

Every downstream query would:
- Recompute expensive OMS/VEMS joins
- Recalculate DQ scores
- Re-execute conflict resolution logic
- **Unacceptable performance**

### Option 2: Materialized Views âŒ

**Limitations block your use case:**
- âŒ No `UNION` (you merge OMS + VEMS)
- âŒ Limited complexity (your logic is extensive)
- âŒ Can't query views (restricts flexibility)
- âŒ Less control over refresh timing

### Option 3: Dynamic Tables âœ… (Implemented for Simple Components)

**Modern Snowflake approach:**
- âœ… Automatic refresh on source changes
- âœ… Built-in incremental processing (CDC)
- âœ… Declarative SQL (no procedural code)
- âœ… Dependency management (DAG)
- âœ… Target lag SLA (e.g., "30 minutes")

**Best for:**
- Reference data (facilities)
- Fact tables (appointments, QA events)
- Real-time requirements
- New pipelines

**Not suitable for:**
- Complex multi-step business logic (veterans, evaluators)
- Heavy audit requirements
- Explicit batch_id tracking

---

## Dynamic Tables Implementation

We've implemented Dynamic Tables for the **simpler** components of your staging layer:

### Components Delivered

| Component | Type | Purpose | Target Lag |
|-----------|------|---------|------------|
| `dt_crosswalk_facilities` | Reference | Facility matching | 15 minutes |
| `dt_stg_facilities` | Staging | Merged facilities | 30 minutes |
| `dt_stg_fact_appointment_events` | Fact | Appointment events | 20 minutes |
| `dt_stg_fact_qa_events` | Fact | QA events | 30 minutes |
| `dt_vw_staging_dq_summary` | Dashboard | Real-time DQ metrics | 10 minutes |

### File Structure

```
snowflake/staging/
  â”œâ”€â”€ 01_create_staging_tables.sql              # Existing (physical tables)
  â”œâ”€â”€ 02_staging_layer_oms_vems_merge_simplified.sql  # Existing (stored procedures)
  â””â”€â”€ 03_dynamic_tables_staging_layer.sql       # NEW (Dynamic Tables)

snowflake/monitoring/
  â”œâ”€â”€ staging_layer_validation_queries.sql      # Existing
  â””â”€â”€ dynamic_tables_monitoring.sql             # NEW (monitoring queries)

DYNAMIC_TABLES_IMPLEMENTATION_GUIDE.md          # NEW (comprehensive guide)
SNOWFLAKE_STAGING_LAYER_REVIEW.md              # THIS FILE
```

### Architecture: Hybrid Approach

```
ODS_RAW
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DYNAMIC TABLES (Simple)         â”‚
â”‚ - Facilities                    â”‚
â”‚ - Appointment Events            â”‚
â”‚ - QA Events                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STORED PROCEDURES (Complex)     â”‚
â”‚ - Veterans (SSN matching)       â”‚
â”‚ - Evaluators (NPI matching)     â”‚
â”‚ - Heavy conflict resolution     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
WAREHOUSE (Star Schema)
```

---

## Benefits of Dynamic Tables Approach

### 1. No Manual Orchestration

**Before (Stored Procedures):**
```sql
CALL sp_staging_layer_master('BATCH_2025_11_20_001');
```

**After (Dynamic Tables):**
- Automatic refresh when ODS data changes
- No scheduling needed
- No batch_id management

### 2. Automatic Incremental Processing

**Before:**
```sql
DELETE FROM stg_facilities WHERE batch_id = :batch_id;
INSERT INTO stg_facilities SELECT /* merge logic */;
```

**After:**
- Snowflake handles CDC automatically
- Only processes changed data
- More efficient compute usage

### 3. Declarative SQL

**Before:** 8 stored procedures, ~800 lines of procedural code

**After:** 5 declarative SQL queries, self-documenting

### 4. Real-Time Capability

**Before:** Batch runs (e.g., 4x daily)

**After:** Near real-time with `TARGET_LAG = '15 minutes'`

### 5. Simplified Monitoring

**Built-in views:**
- `INFORMATION_SCHEMA.DYNAMIC_TABLES()`
- `INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY()`

Plus custom monitoring views:
- `vw_dynamic_table_health`
- `vw_dynamic_table_refresh_history`
- `vw_dynamic_table_performance_trends`

---

## When to Use Each Approach

### Use Physical Tables + Stored Procedures

âœ… Veterans staging (complex SSN matching, high conflicts)
âœ… Evaluators staging (complex NPI matching, system-of-record rules)
âœ… Heavy audit requirements (write to reconciliation log)
âœ… Explicit batch tracking needed
âœ… Complex error handling (TRY/CATCH)

### Use Dynamic Tables

âœ… Facilities (reference data, low complexity)
âœ… Fact tables (appointments, QA events)
âœ… Data quality dashboards
âœ… Real-time requirements (< 30 min lag)
âœ… New pipelines starting from scratch

### Use Regular Views

âœ… Convenience wrappers (hide batch_id complexity)
âœ… Business logic abstraction
âœ… Calculated fields on top of staging

---

## Cost Considerations

### Stored Procedures

**Cost Model:**
- Pay only when procedure runs (e.g., 4x daily)
- Full refresh each time
- Predictable compute usage

**Best for:** Infrequent updates (< 4x daily)

### Dynamic Tables

**Cost Model:**
- Pay for continuous refresh operations
- Incremental processing (more efficient)
- Storage for materialized data

**Best for:** Frequent updates (every 15-60 minutes) or real-time needs

### Optimization Strategies

1. **Right-size TARGET_LAG**
   - Business priority vs. cost trade-off
   - Not all tables need 15-minute freshness

2. **Leverage Incremental Mode**
   - Avoid non-deterministic functions
   - Use `UNION ALL` instead of `UNION`
   - Time-based filtering with static lookback

3. **Monitor Compute Usage**
   - Track refresh duration trends
   - Alert on performance degradation
   - Optimize slow queries

---

## Implementation Roadmap

### Phase 1: Deploy Dynamic Tables (Week 1) âœ… DONE

- [x] Create `dt_crosswalk_facilities`
- [x] Create `dt_stg_facilities`
- [x] Create `dt_stg_fact_appointment_events`
- [x] Create `dt_stg_fact_qa_events`
- [x] Create `dt_vw_staging_dq_summary`
- [x] Create monitoring views
- [x] Write implementation guide

### Phase 2: Validate (Week 2)

- [ ] Run in parallel with stored procedures
- [ ] Compare data quality and completeness
- [ ] Monitor performance and costs
- [ ] Validate downstream consumers work correctly

### Phase 3: Cutover (Week 3)

- [ ] Point downstream consumers to Dynamic Tables
- [ ] Stop calling stored procedures for facilities/facts
- [ ] Monitor closely for issues
- [ ] Keep stored procedures as backup

### Phase 4: Expand (Week 4+) - Optional

- [ ] Evaluate veterans/evaluators for Dynamic Tables
- [ ] May require redesign due to complexity
- [ ] Cost-benefit analysis
- [ ] Keep stored procedures if more appropriate

---

## Key Takeaways

### 1. Your Current Design is Correct âœ…

Physical tables with stored procedures are appropriate for your complex OMS/VEMS integration. Don't change what's working.

### 2. No Universal Rule on Views âŒ

Snowflake does **not** mandate views/materialized views for staging layers. The choice depends on your specific requirements.

### 3. Dynamic Tables: Modern Alternative âœ…

We've implemented Dynamic Tables for simpler components, giving you:
- Automatic refresh
- Built-in CDC
- Declarative SQL
- Real-time capability

### 4. Hybrid Approach is Best ğŸ¯

- **Dynamic Tables**: Simple entities, facts, reference data
- **Stored Procedures**: Complex business logic, heavy audit needs

### 5. Start Small, Validate, Expand ğŸ“ˆ

- Begin with low-risk tables (facilities) âœ…
- Validate results in parallel
- Gradually expand to more components
- Keep stored procedures for complex cases

---

## Recommendations

### Immediate (Do Now)

1. âœ… **Review Dynamic Tables implementation** in `03_dynamic_tables_staging_layer.sql`
2. âœ… **Read implementation guide** in `DYNAMIC_TABLES_IMPLEMENTATION_GUIDE.md`
3. **Deploy to DEV environment** for testing
4. **Run validation queries** to compare with stored procedure output

### Short-Term (Next 2-4 Weeks)

1. **Parallel run** Dynamic Tables alongside stored procedures
2. **Monitor performance** and costs using monitoring queries
3. **Validate data quality** matches stored procedure approach
4. **Cut over** to Dynamic Tables for facilities and facts

### Long-Term (Next Quarter)

1. **Evaluate veterans/evaluators** for Dynamic Tables migration
2. **Optimize TARGET_LAG** based on business needs vs. costs
3. **Expand to new entities** using Dynamic Tables pattern
4. **Retire stored procedures** for migrated components (keep as backup)

---

## Questions?

### Technical Questions

- **Dynamic Tables syntax**: See `03_dynamic_tables_staging_layer.sql`
- **Monitoring setup**: See `dynamic_tables_monitoring.sql`
- **Implementation steps**: See `DYNAMIC_TABLES_IMPLEMENTATION_GUIDE.md`

### Architecture Questions

- **Why not all Dynamic Tables?**: Complex business logic better in stored procedures
- **When to use which approach?**: See "When to Use Each Approach" section above
- **How to migrate existing tables?**: See implementation guide Phase 2-4

### Operational Questions

- **How to monitor health?**: Use `vw_dynamic_table_health` view
- **How to troubleshoot failures?**: See implementation guide troubleshooting section
- **How to optimize costs?**: See "Cost Considerations" section above

---

## Conclusion

Your question about using only views/materialized views in staging revealed an opportunity to modernize your architecture. While your current physical tables + stored procedures approach is **correct and should be maintained for complex processing**, Dynamic Tables offer a **modern alternative for simpler components**.

We've implemented a **hybrid approach**:
- **Keep stored procedures** for veterans/evaluators (complex)
- **Use Dynamic Tables** for facilities/facts (simple)
- **Best of both worlds**: Simplicity where possible, control where needed

This aligns with Snowflake best practices while meeting your specific business requirements.

---

**Review Conducted By:** Claude (AI Assistant)
**Date:** 2025-11-20
**Status:** Implementation Complete âœ…
**Next Steps:** Deploy to DEV, validate, cutover to production

---

## References

### Snowflake Documentation

- [Dynamic Tables Overview](https://docs.snowflake.com/en/user-guide/dynamic-tables-about)
- [Medallion Architecture](https://www.snowflake.com/guides/what-medallion-architecture)
- [ELT Best Practices](https://www.snowflake.com/guides/elt-best-practices)

### Internal Documentation

- `DATA_PIPELINE_ARCHITECTURE.md` - Overall system design
- `STAGING_LAYER_IMPLEMENTATION_GUIDE.md` - Stored procedure approach
- `DYNAMIC_TABLES_IMPLEMENTATION_GUIDE.md` - Dynamic Tables approach (NEW)

### Code Files

- `snowflake/staging/03_dynamic_tables_staging_layer.sql` - Dynamic Tables DDL (NEW)
- `snowflake/monitoring/dynamic_tables_monitoring.sql` - Monitoring queries (NEW)
